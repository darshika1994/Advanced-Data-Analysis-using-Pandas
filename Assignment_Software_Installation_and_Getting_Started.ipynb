{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshika1994/Advanced-Data-Analysis-using-Pandas/blob/master/Assignment_Software_Installation_and_Getting_Started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Installing and Importing Spacy Library**"
      ],
      "metadata": {
        "id": "U7wCKQO133y5"
      },
      "id": "U7wCKQO133y5"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8376caa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8376caa3",
        "outputId": "102decd3-8ef2-48b9-c199-5498a685dc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e218af4",
      "metadata": {
        "id": "4e218af4"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Loading Spacy English Language Model & Processing Text from News Article**"
      ],
      "metadata": {
        "id": "0WCbKW844EjY"
      },
      "id": "0WCbKW844EjY"
    },
    {
      "cell_type": "markdown",
      "id": "959d86cf",
      "metadata": {
        "id": "959d86cf"
      },
      "source": [
        "*The model \"en_core_web_sm\" is a small English Language Model provided by spaCy for tasks like tokenization, POS tagging, and dependency parsing. To use this model, we need to download and install the 'en_core_web_sm' model using spaCy's download command :*\n",
        "\n",
        "**`python -m spacy download en_core_web_sm`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c2e796b",
      "metadata": {
        "id": "9c2e796b"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "bfba03ae",
      "metadata": {
        "id": "bfba03ae"
      },
      "outputs": [],
      "source": [
        "processed_text = nlp(\"Tech giants faced scrutiny as regulators announced a probe into potential antitrust violations, signaling a new era of scrutiny for big tech.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c640bd55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c640bd55",
        "outputId": "456db658-cca4-42ba-8880-bd2eee0f672a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tech tech NOUN NN compound True False\n",
            "giants giant NOUN NNS nsubj True False\n",
            "faced face VERB VBD ROOT True False\n",
            "scrutiny scrutiny NOUN NN dobj True False\n",
            "as as SCONJ IN mark True True\n",
            "regulators regulator NOUN NNS nsubj True False\n",
            "announced announce VERB VBD advcl True False\n",
            "a a DET DT det True True\n",
            "probe probe NOUN NN dobj True False\n",
            "into into ADP IN prep True True\n",
            "potential potential ADJ JJ amod True False\n",
            "antitrust antitrust ADJ JJ amod True False\n",
            "violations violation NOUN NNS pobj True False\n",
            ", , PUNCT , punct False False\n",
            "signaling signal VERB VBG advcl True False\n",
            "a a DET DT det True True\n",
            "new new ADJ JJ amod True False\n",
            "era era NOUN NN dobj True False\n",
            "of of ADP IN prep True True\n",
            "scrutiny scrutiny NOUN NN pobj True False\n",
            "for for ADP IN prep True True\n",
            "big big ADJ JJ amod True False\n",
            "tech tech NOUN NN pobj True False\n",
            ". . PUNCT . punct False False\n"
          ]
        }
      ],
      "source": [
        "for token in processed_text:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Examing Spacy Output**"
      ],
      "metadata": {
        "id": "3n_RFQeM5vTl"
      },
      "id": "3n_RFQeM5vTl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>1. Did all the words break apart in a way that you expected it to â€“ we asked spacy to identify all the separate words in a sentence. Is that what it did? </font>\n",
        "\n",
        "Yes, this code breaks apart the text into separate words/tokens and provides various linguistic attributes for each word like it's part of speech tag, whether it's a stop word, which is what wou would expect from using spaCy for tokenization."
      ],
      "metadata": {
        "id": "FGxPCbVVJ-0a"
      },
      "id": "FGxPCbVVJ-0a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>2. Does there appear to be a common part of speech from your words? What does that imply? </font>\n",
        "\n",
        "From the output of the code, it is worth noting that many of the words share the same part-of-speech tag, specifically NOUN (noun).\n",
        "The fact that many words are tagged as nouns suggests that nouns dominate the parts of speech in this particular sentence. Nouns typically represent people, places, things, or ideas, and they often serve as the subject or object of a sentence."
      ],
      "metadata": {
        "id": "OChNTlIpKEAf"
      },
      "id": "OChNTlIpKEAf"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "pos_tags = [token.pos_ for token in processed_text]\n",
        "pos_frequency = Counter(pos_tags)\n",
        "pos_frequency_sorted = pos_frequency.most_common()\n",
        "print(pos_frequency_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_DqRPrVKKLT",
        "outputId": "23d9da62-0254-4c6d-9507-eda86a7b8f53"
      },
      "id": "R_DqRPrVKKLT",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NOUN', 9), ('ADJ', 4), ('VERB', 3), ('ADP', 3), ('DET', 2), ('PUNCT', 2), ('SCONJ', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we can see that maximum times noun (i.e. 9) have appeared in the processed text followed by adjective."
      ],
      "metadata": {
        "id": "huZYa1pBKYIu"
      },
      "id": "huZYa1pBKYIu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>3. Examining the last column (is_stop) what kinds of words do you expect stop words to be? Do you think they will be useful for text analysis? </font>\n",
        "\n",
        "Stop words are basically the little words in a sentence like \"the,\" \"and,\" \"is,\" \"of,\" etc. They're important for putting sentences together correctly, but they don't add much meaning on their own and are insignificant. When we check the last column (is_stop), we can see words like \"as,\" \"a,\" \"for,\" \"into,\" \"of,\" etc. They're essential for making sentences work, but they're not very useful for understanding the main point or feeling of a piece of text."
      ],
      "metadata": {
        "id": "H3r_qaDLKPn3"
      },
      "id": "H3r_qaDLKPn3"
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = []\n",
        "\n",
        "for token in processed_text:\n",
        "    if token.is_stop:\n",
        "        stop_words.append(token.text)\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1Cj3B9oKSg2",
        "outputId": "3014ffbf-b327-4980-fd42-83c444fdf14a"
      },
      "id": "w1Cj3B9oKSg2",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['as', 'a', 'into', 'a', 'of', 'for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are stop words in the processed text i.e. \"as\", \"a\",\"for\". That's why in tasks like figuring out what a piece of writing is about or how people feel about it, it's often a good practice to ignore or get rid of these stop words."
      ],
      "metadata": {
        "id": "OctKs366I_hG"
      },
      "id": "OctKs366I_hG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Examine Spacy Output 2**"
      ],
      "metadata": {
        "id": "OcD2t-ZuBwaC"
      },
      "id": "OcD2t-ZuBwaC"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "240d04ad",
      "metadata": {
        "id": "240d04ad"
      },
      "outputs": [],
      "source": [
        "processed_text = nlp(\"Tech giants faced scrutiny as regulators announced a probe into potential antitrust violations, signaling a new era of scrutiny for big tech.\")\n",
        "for ent in processed_text.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our text above, there are no named entities recognized by spaCy, which is we are not getting any output. Named entities are specific entities like people, organizations, locations, dates, etc., recognized by the NER (Named Entity Recognition) component of the spaCy model. But in our sentence, there aren't any such named entities (like people, location, dates etc) present.\n",
        "\n",
        "*So, using a different text which has named entities in it :*"
      ],
      "metadata": {
        "id": "jtVsKKQyDHjF"
      },
      "id": "jtVsKKQyDHjF"
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text = nlp(\"Elon Musk's SpaceX successfully launched a new satellite into orbit from their facility in Cape Canaveral, Florida.\")\n",
        "for ent in processed_text.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVrc3NsICj93",
        "outputId": "a47e4e9b-a003-468c-fd0a-a1da7af26b6d"
      },
      "id": "TVrc3NsICj93",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk's 0 11 PERSON\n",
            "Cape Canaveral 91 105 GPE\n",
            "Florida 107 114 GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3683742",
      "metadata": {
        "id": "c3683742"
      },
      "source": [
        "<font color='green'>1. Given the output, what do you think named entities are? </font>\n",
        "\n",
        "As already mentioned above, Named entities are specific entities like people, organizations, locations, dates, etc., recognized by the NER (Named Entity Recognition) component of the spaCy model. In the above txt, the named entities are individuals (such as \"Elon Musk\"), locations (such as \"Cape Canaveral\" and \"Florida\").\n",
        "\n",
        "<font color='green'>2. How does the output differ from the text provided by printing the parts of speech? </font>\n",
        "\n",
        "Named entities selects specific things like people or places, while parts of speech tell you about the grammar of each word. Named entities give you the big picture, like \"Elon Musk\" or \"SpaceX,\" while parts of speech dive into the details, like whether a word is a noun or a verb, as we have seen in previous example of Tecg giants. Named entities cover only certain important words, while parts of speech cover every word in the text. Both are useful for understanding text, but they focus on different aspects: named entities are particularly useful for tasks involving entity recognition, extraction and parts of speech are essential for syntactic parsing & grammatical analysis.\n",
        "\n",
        "In short, Named entity labels (e.g., PERSON, ORGANIZATION, GPE) categorize named entities into broad semantic categories, while part-of-speech tags (e.g., NOUN, VERB, ADJECTIVE) classify words based on their grammatical roles.\n",
        "\n",
        "<font color='green'>3. What might you guess spacy tends to label in named entity recognition? </font>\n",
        "\n",
        "SpaCy's named entity recognition (NER) typically labels entities that represent specific types of information within text. Some common categories of entities that spaCy tends to label include:\n",
        "\n",
        "*   PERSON: Names of people or characters.\n",
        "*   ORGANIZATION: Names of companies, institutions, or groups.\n",
        "*   GPE (Geopolitical Entity): Names of countries, cities, states, or other geopolitical entities.\n",
        "*   DATE: Specific dates or time expressions.\n",
        "*   TIME: Specific times or time expressions.\n",
        "*   PRODUCT: Names of products or items.\n",
        "*   EVENT: Names of events or occurrences.\n",
        "*   MONEY: Monetary values or currency symbols.\n",
        "*   QUANTITY: Measurements or numerical quantities.\n",
        "*   FACILITY: Names of buildings, airports, or other facilities.\n",
        "\n",
        "\n",
        "In the text above, we have got **person (elon musk) and gpe (cape canaveral and florida) as output** which are the named entities. The exact categories might change depending on the language and version of the spaCy model you're using, but the main goal is to find and label important stuff in the text."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}